{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential, Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/'\n",
    "allfiles = glob.glob(path+'sensory/tile/*.npy')\n",
    "sali = glob.glob(path+'content/saliencyImages/*.npy')\n",
    "motion = glob.glob(path+'content/motionImages/*.npy')\n",
    "prob = glob.glob(path+'sensory/tileProb/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/'\n",
    "allfiles = glob.glob(path+'sensory/tile/*.npy')\n",
    "sali = glob.glob(path+'content/saliencyImages/*.npy')\n",
    "motion = glob.glob(path+'content/motionImages/*.npy')\n",
    "prob = glob.glob(path)\n",
    "\n",
    "def myGenerator():\n",
    "    while True:\n",
    "        index_list = random.sample(range(1, 30000), 32)\n",
    "        alldata_x = []\n",
    "        alldata_y = []\n",
    "        for i in index_list:\n",
    "            f = allfiles[i]\n",
    "            s = f.split('_')\n",
    "            saliFile = '../datasets/content/saliencyImages/'+s[0][25:]+'_saliency_'+s[2].split('.')[0]+'.npy'\n",
    "            motionFile = '../datasets/content/motionImages/'+s[0][25:]+'_motion_'+s[2].split('.')[0]+'.npy'\n",
    "            probFile = '../datasets/sensory/tileProb/'+s[0][25:]+'_user'+s[1][4:]+'_'+s[2].split('.')[0]+'.npy'\n",
    "            a = np.load(f)\n",
    "            b = np.load(saliFile)\n",
    "            c = np.load(motionFile)\n",
    "            d = [a, b, c]\n",
    "            alldata_x.append(d)\n",
    "            alldata_y.append(np.load(probFile))\n",
    "        alldata_x = np.array(alldata_x)\n",
    "        alldata_x = np.rollaxis(alldata_x, 1, 5)  \n",
    "        #alldata_x = alldata_x.reshape((32, 30, 240, 480, 3))\n",
    "        #alldata_x = np.swapaxes(alldata_x, 1, 4)\n",
    "        alldata_y = np.array(alldata_y)\n",
    "        yield alldata_x, alldata_y\n",
    "# x = myGenerator()\n",
    "# xtrain, ytrain = next(x)\n",
    "# print('xtrain shape:',xtrain.shape)\n",
    "# print('ytrain shape:',ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb8534c8f28>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8b35fb828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(xtrain[0, 0][:, :, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/sensory/tile/landscape_user09_54.npy\n",
      "['../datasets/sensory/tile/landscape', 'user09', '54.npy']\n",
      "../datasets/sensory/tileProb/landscape_user09_54.npy\n"
     ]
    }
   ],
   "source": [
    "print(allfiles[1])\n",
    "f = allfiles[1]\n",
    "s = f.split('_')\n",
    "print(s)\n",
    "print('../datasets/sensory/tileProb/'+s[0][25:]+'_user'+s[1][4:]+'_'+s[2].split('.')[0]+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/'\n",
    "videoNames = os.listdir(path+'content/saliency/')\n",
    "videoNames = [i[:-13] for i in videoNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numpy arrays from saliency, motion maps and sensor data\n",
    "sali = glob.glob(path+'content/saliencyImages/*.npy')\n",
    "motion = glob.glob(path+'content/motionImages/*.npy')\n",
    "sensory = glob.glob(path+'sensory/tile/*.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Flatten, Reshape\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.applications import imagenet_utils\n",
    "\n",
    "input_shape=(30, 240, 480, 3)\n",
    "def mySegNet(input_shape):\n",
    "    base_model  = MobileNet(input_shape=(224,224,3), include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    cnn_model = Model(inputs=base_model.input, outputs=x)\n",
    "    \n",
    "    model = Sequential();\n",
    "    model.add(TimeDistributed(cnn_model, input_shape=input_shape))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    \n",
    "    model.add(LSTM(200, return_sequences=True))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    #print(model.summary())\n",
    "    return model\n",
    "    \n",
    "\n",
    "#mySegNet(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(epochs=10, steps_per_epoch=50, generator=<generator..., use_multiprocessing=True)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[960,240,480,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_1/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_1/Reshape, block1_conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2981_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'time_distributed_1/block1_conv1/convolution', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python3/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python3/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-be7cac9378cf>\", line 2, in <module>\n    model = mySegNet(input_shape)\n  File \"<ipython-input-3-80d72c3b6a9d>\", line 21, in mySegNet\n    model.add(TimeDistributed(cnn_model, input_shape=input_shape))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/models.py\", line 467, in add\n    layer(x)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/wrappers.py\", line 203, in call\n    y = self.layer.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2078, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2229, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 3332, in conv2d\n    data_format=tf_data_format)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 754, in convolution\n    return op(input, filter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 838, in __call__\n    return self.conv_op(inp, filter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 502, in __call__\n    return self.call(inp, filter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 190, in __call__\n    name=self.name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 639, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[960,240,480,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_1/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_1/Reshape, block1_conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2981_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[960,240,480,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_1/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_1/Reshape, block1_conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2981_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-be7cac9378cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit_generator(generator=myGenerator(),\n\u001b[1;32m      4\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    steps_per_epoch=50, nb_epoch=10)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[960,240,480,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_1/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_1/Reshape, block1_conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2981_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'time_distributed_1/block1_conv1/convolution', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/lib/python3/dist-packages/tornado/ioloop.py\", line 866, in start\n    handler_func(fd_obj, events)\n  File \"/usr/lib/python3/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-be7cac9378cf>\", line 2, in <module>\n    model = mySegNet(input_shape)\n  File \"<ipython-input-3-80d72c3b6a9d>\", line 21, in mySegNet\n    model.add(TimeDistributed(cnn_model, input_shape=input_shape))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/models.py\", line 467, in add\n    layer(x)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/wrappers.py\", line 203, in call\n    y = self.layer.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2078, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\", line 2229, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 3332, in conv2d\n    data_format=tf_data_format)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 754, in convolution\n    return op(input, filter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 838, in __call__\n    return self.conv_op(inp, filter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 502, in __call__\n    return self.call(inp, filter)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py\", line 190, in __call__\n    name=self.name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 639, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[960,240,480,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: time_distributed_1/block1_conv1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](time_distributed_1/Reshape, block1_conv1/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_409 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2981_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "input_shape=(30, 240, 480, 3)\n",
    "model = mySegNet(input_shape)\n",
    "model.fit_generator(generator=myGenerator(),\n",
    "                    use_multiprocessing=True,\n",
    "                   steps_per_epoch=50, nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_model  = VGG16(input_shape=(224,224,3), include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "cnn_model = Model(inputs=base_model.input, outputs=x)\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['landscape', 'pacman', 'panel', 'ride', 'sport']\n",
      "(150, 1920, 3840)\n"
     ]
    }
   ],
   "source": [
    "videoNames = sorted(videoNames)[5:]\n",
    "print(videoNames)\n",
    "#print(sorted(sali))\n",
    "\n",
    "for video in videoNames:\n",
    "    npys = [s for s in sali if video in s]\n",
    "    for npy in npys:\n",
    "        data = np.load(npy)\n",
    "        print(data.shape)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pre-trained VGG model\n",
    "def loadVGG16Model():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    #print (\"Model loaded..!\")\n",
    "    #print (base_model.summary())\n",
    "    return base_model\n",
    "vgg_model = loadVGG16Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f4ae05e9a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getBaseModel():\n",
    "    #base_model  = MobileNet(input_shape=(224,224,3), include_top=False)\n",
    "    #base_model  = ResNet50(input_shape=(224,224,3), include_top=False)\n",
    "    base_model  = VGG16(input_shape=(224,224,3), include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    sgd = optimizers.SGD(lr=0.0001)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    print (model.summary())\n",
    "    return model\n",
    "getBaseModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(): \n",
    "    nFilters=32\n",
    "    kernelSize=(3,3)\n",
    "    poolSize=(2,2)\n",
    "    batchSize=64\n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(40, (3, 3), activation='relu'), input_shape=[224, 224, 1]))\n",
    "    #model.add(TimeDistributed(Conv2D(nFilters, kernel_size = kernelSize, activation=\"relu\"), input_shape=[1920, 3840,1]))\n",
    "    model.add(TimeDistributed(Conv2D(nFilters*2, kernel_size = kernelSize, activation=\"relu\")))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=poolSize)))\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(nFilters, kernel_size = kernelSize, activation=\"relu\")))\n",
    "    model.add(TimeDistributed(Conv2D(nFilters*2, kernel_size = kernelSize, activation=\"relu\")))\n",
    "    model.add(TimeDistributed(MaxPooling2D(pool_size=poolSize)))\n",
    "\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(5))\n",
    "    #model.add(Dense(, input_dim=, activation='relu'))\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model\n",
    "model = buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
